{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.tensor.shared_randomstreams import RandomStreams\n",
    "from lasagne import init\n",
    "from lasagne.layers import (Layer, InputLayer, MergeLayer, DenseLayer, \n",
    "                            DimshuffleLayer, ElemwiseSumLayer, \n",
    "                            ReshapeLayer, NonlinearityLayer, \n",
    "                            get_all_params, get_output)\n",
    "from lasagne.objectives import categorical_crossentropy\n",
    "from lasagne.nonlinearities import rectify, sigmoid, softmax\n",
    "from lasagne.updates import total_norm_constraint, adam\n",
    "from parmesan.layers import SampleLayer\n",
    "from parmesan.distributions import log_normal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ADGMSSL():\n",
    "    \"\"\"\n",
    "    The :class:'ADGMSSL' class represents the implementation of the model described in\n",
    "    http://approximateinference.org/accepted/MaaloeEtAl2015.pdf.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_x, n_a, n_z, n_y, a_hidden, z_hidden, xhat_hidden, y_hidden, trans_func=rectify,\n",
    "                 x_dist='bernoulli'):\n",
    "        \"\"\"\n",
    "        Initialize an auxiliary deep generative model consisting of\n",
    "        discriminative classifier q(y|a,x),\n",
    "        generative model P p(xhat|z,y),\n",
    "        inference model Q q(a|x) and q(z|x,y).\n",
    "        All weights are initialized using the Bengio and Glorot (2010) initialization scheme.\n",
    "        :param n_x: Number of inputs.\n",
    "        :param n_a: Number of auxiliary.\n",
    "        :param n_z: Number of latent.\n",
    "        :param n_y: Number of classes.\n",
    "        :param a_hidden: List of number of deterministic hidden q(a|x).\n",
    "        :param z_hidden: List of number of deterministic hidden q(z|x,y).\n",
    "        :param xhat_hidden: List of number of deterministic hidden p(xhat|z,y).\n",
    "        :param y_hidden: List of number of deterministic hidden q(y|a,x).\n",
    "        :param trans_func: The transfer function used in the deterministic layers.\n",
    "        :param x_dist: The x distribution, 'bernoulli' or 'gaussian'.\n",
    "        \"\"\"\n",
    "        self.y_hidden = y_hidden\n",
    "        self.x_dist = x_dist\n",
    "        self.n_y = n_y\n",
    "        self.n_x = n_x\n",
    "        self.n_a = n_a\n",
    "        self.n_z = n_z\n",
    "        self.transf = trans_func\n",
    "\n",
    "        self._srng = RandomStreams()\n",
    "\n",
    "        self.sym_beta = T.scalar('beta')  # symbolic upscaling of the discriminative term.\n",
    "        self.sym_x_l = T.matrix('x')  # symbolic labeled inputs\n",
    "        self.sym_t_l = T.matrix('t')  # symbolic labeled targets\n",
    "        self.sym_x_u = T.matrix('x')  # symbolic unlabeled inputs\n",
    "        self.sym_bs_l = T.iscalar('bs_l')  # symbolic number of labeled data_preparation points in batch\n",
    "        self.sym_samples = T.iscalar('samples')  # symbolic number of Monte Carlo samples\n",
    "        self.sym_y = T.matrix('y')\n",
    "        self.sym_z = T.matrix('z')\n",
    "\n",
    "        ### Input layers ###\n",
    "        l_x_in = InputLayer((None, n_x))\n",
    "        l_y_in = InputLayer((None, n_y))\n",
    "\n",
    "        ### Auxiliary q(a|x) ###\n",
    "        l_a_x = l_x_in\n",
    "        for hid in a_hidden:\n",
    "            l_a_x = DenseLayer(l_a_x, hid, init.GlorotNormal('relu'), init.Normal(1e-3), self.transf)\n",
    "        l_a_x_mu = DenseLayer(l_a_x, n_a, init.GlorotNormal(), init.Normal(1e-3), None)\n",
    "        l_a_x_logvar = DenseLayer(l_a_x, n_a, init.GlorotNormal(), init.Normal(1e-3), None)\n",
    "        l_a_x = SampleLayer(l_a_x_mu, l_a_x_logvar, eq_samples=self.sym_samples)\n",
    "        # Reshape all layers to align them for multiple samples in the lower bound calculation.\n",
    "        l_a_x_reshaped = ReshapeLayer(l_a_x, (-1, self.sym_samples, 1, n_a))\n",
    "        l_a_x_mu_reshaped = DimshuffleLayer(l_a_x_mu, (0, 'x', 'x', 1))\n",
    "        l_a_x_logvar_reshaped = DimshuffleLayer(l_a_x_logvar, (0, 'x', 'x', 1))\n",
    "\n",
    "        ### Classifier q(y|a,x) ###\n",
    "        # Concatenate the input x and the output of the auxiliary MLP.\n",
    "        l_a_to_y = DenseLayer(l_a_x, y_hidden[0], init.GlorotNormal('relu'), init.Normal(1e-3), None)\n",
    "        l_a_to_y = ReshapeLayer(l_a_to_y, (-1, self.sym_samples, 1, y_hidden[0]))\n",
    "        l_x_to_y = DenseLayer(l_x_in, y_hidden[0], init.GlorotNormal('relu'), init.Normal(1e-3), None)\n",
    "        l_x_to_y = DimshuffleLayer(l_x_to_y, (0, 'x', 'x', 1))\n",
    "        l_y_xa = ReshapeLayer(ElemwiseSumLayer([l_a_to_y, l_x_to_y]), (-1, y_hidden[0]))\n",
    "        l_y_xa = NonlinearityLayer(l_y_xa, self.transf)\n",
    "\n",
    "        if len(y_hidden) > 1:\n",
    "            for hid in y_hidden[1:]:\n",
    "                l_y_xa = DenseLayer(l_y_xa, hid, init.GlorotUniform('relu'), init.Normal(1e-3), self.transf)\n",
    "        l_y_xa = DenseLayer(l_y_xa, n_y, init.GlorotUniform(), init.Normal(1e-3), softmax)\n",
    "        l_y_xa_reshaped = ReshapeLayer(l_y_xa, (-1, self.sym_samples, 1, n_y))\n",
    "\n",
    "        ### Recognition q(z|x,y) ###\n",
    "        # Concatenate the input x and y.\n",
    "        l_x_to_z = DenseLayer(l_x_in, z_hidden[0], init.GlorotNormal('relu'), init.Normal(1e-3), None)\n",
    "        l_x_to_z = DimshuffleLayer(l_x_to_z, (0, 'x', 'x', 1))\n",
    "        l_y_to_z = DenseLayer(l_y_in, z_hidden[0], init.GlorotNormal('relu'), init.Normal(1e-3), None)\n",
    "        l_y_to_z = DimshuffleLayer(l_y_to_z, (0, 'x', 'x', 1))\n",
    "        l_z_xy = ReshapeLayer(ElemwiseSumLayer([l_x_to_z, l_y_to_z]), [-1, z_hidden[0]])\n",
    "        l_z_xy = NonlinearityLayer(l_z_xy, self.transf)\n",
    "\n",
    "        if len(z_hidden) > 1:\n",
    "            for hid in z_hidden[1:]:\n",
    "                l_z_xy = DenseLayer(l_z_xy, hid, init.GlorotNormal('relu'), init.Normal(1e-3), self.transf)\n",
    "        l_z_axy_mu = DenseLayer(l_z_xy, n_z, init.GlorotNormal(), init.Normal(1e-3), None)\n",
    "        l_z_axy_logvar = DenseLayer(l_z_xy, n_z, init.GlorotNormal(), init.Normal(1e-3), None)\n",
    "        l_z_xy = SampleLayer(l_z_axy_mu, l_z_axy_logvar, eq_samples=self.sym_samples)\n",
    "        # Reshape all layers to align them for multiple samples in the lower bound calculation.\n",
    "        l_z_axy_mu_reshaped = DimshuffleLayer(l_z_axy_mu, (0, 'x', 'x', 1))\n",
    "        l_z_axy_logvar_reshaped = DimshuffleLayer(l_z_axy_logvar, (0, 'x', 'x', 1))\n",
    "        l_z_axy_reshaped = ReshapeLayer(l_z_xy, (-1, self.sym_samples, 1, n_z))\n",
    "\n",
    "        ### Generative p(xhat|z,y) ###\n",
    "        # Concatenate the input x and y.\n",
    "        l_y_to_xhat = DenseLayer(l_y_in, xhat_hidden[0], init.GlorotNormal('relu'), init.Normal(1e-3), None)\n",
    "        l_y_to_xhat = DimshuffleLayer(l_y_to_xhat, (0, 'x', 'x', 1))\n",
    "        l_z_to_xhat = DenseLayer(l_z_xy, xhat_hidden[0], init.GlorotNormal('relu'), init.Normal(1e-3), None)\n",
    "        l_z_to_xhat = ReshapeLayer(l_z_to_xhat, (-1, self.sym_samples, 1, xhat_hidden[0]))\n",
    "        l_xhat_zy = ReshapeLayer(ElemwiseSumLayer([l_z_to_xhat, l_y_to_xhat]), [-1, xhat_hidden[0]])\n",
    "        l_xhat_zy = NonlinearityLayer(l_xhat_zy, self.transf)\n",
    "        if len(xhat_hidden) > 1:\n",
    "            for hid in xhat_hidden[1:]:\n",
    "                l_xhat_zy = DenseLayer(l_xhat_zy, hid, init.GlorotNormal('relu'), init.Normal(1e-3), self.transf)\n",
    "        if x_dist == 'bernoulli':\n",
    "            l_xhat_zy_mu_reshaped = None\n",
    "            l_xhat_zy_logvar_reshaped = None\n",
    "            l_xhat_zy = DenseLayer(l_xhat_zy, n_x, init.GlorotNormal(), init.Normal(1e-3), sigmoid)\n",
    "        elif x_dist == 'gaussian':\n",
    "            l_xhat_zy_mu = DenseLayer(l_xhat_zy, n_x, init.GlorotNormal(), init.Normal(1e-3), None)\n",
    "            l_xhat_zy_logvar = DenseLayer(l_xhat_zy, n_x, init.GlorotNormal(), init.Normal(1e-3), None)\n",
    "            l_xhat_zy = SampleLayer(l_xhat_zy_mu, l_xhat_zy_logvar, eq_samples=1)\n",
    "            l_xhat_zy_mu_reshaped = ReshapeLayer(l_xhat_zy_mu, (-1, self.sym_samples, 1, n_x))\n",
    "            l_xhat_zy_logvar_reshaped = ReshapeLayer(l_xhat_zy_logvar, (-1, self.sym_samples, 1, n_x))\n",
    "        l_xhat_zy_reshaped = ReshapeLayer(l_xhat_zy, (-1, self.sym_samples, 1, n_x))\n",
    "\n",
    "        ### Various class variables ###\n",
    "        self.l_x_in = l_x_in\n",
    "        self.l_y_in = l_y_in\n",
    "        self.l_a_mu = l_a_x_mu_reshaped\n",
    "        self.l_a_logvar = l_a_x_logvar_reshaped\n",
    "        self.l_a = l_a_x_reshaped\n",
    "        self.l_z_mu = l_z_axy_mu_reshaped\n",
    "        self.l_z_logvar = l_z_axy_logvar_reshaped\n",
    "        self.l_z = l_z_axy_reshaped\n",
    "        self.l_y = l_y_xa_reshaped\n",
    "        self.l_xhat_mu = l_xhat_zy_mu_reshaped\n",
    "        self.l_xhat_logvar = l_xhat_zy_logvar_reshaped\n",
    "        self.l_xhat = l_xhat_zy_reshaped\n",
    "\n",
    "        self.model_params = get_all_params([self.l_xhat, self.l_y])\n",
    "\n",
    "        ### Predefined functions for generating xhat and y ###\n",
    "        inputs = {l_z_xy: self.sym_z, self.l_y_in: self.sym_y}\n",
    "        outputs = get_output(self.l_xhat, inputs, deterministic=True).mean(axis=(1, 2))\n",
    "        inputs = [self.sym_z, self.sym_y, self.sym_samples]\n",
    "        self.f_xhat = theano.function(inputs, outputs)\n",
    "\n",
    "        inputs = [self.sym_x_l, self.sym_samples]\n",
    "        outputs = get_output(self.l_y, self.sym_x_l, deterministic=True).mean(axis=(1, 2))\n",
    "        self.f_y = theano.function(inputs, outputs)\n",
    "\n",
    "        self.y_params = get_all_params(self.l_y, trainable=True)[(len(a_hidden) + 2) * 2::]\n",
    "        self.xhat_params = get_all_params(self.l_xhat, trainable=True)\n",
    "\n",
    "    def build_model(self, train_set, test_set, validation_set=None):\n",
    "        \"\"\"\n",
    "        Build the auxiliary deep generative model from the initialized hyperparameters.\n",
    "        Define the lower bound term and compile it into a training function.\n",
    "        :param train_set: Train set containing variables x, t.\n",
    "        for the unlabeled data_preparation in the train set, we define 0's in t.\n",
    "        :param test_set: Test set containing variables x, t.\n",
    "        :param validation_set: Validation set containing variables x, t.\n",
    "        :return: train, test, validation function and dicts of arguments.\n",
    "        \"\"\"\n",
    "        self.sym_index = T.iscalar('index')\n",
    "        self.sym_batchsize = T.iscalar('batchsize')\n",
    "        self.sym_lr = T.scalar('learningrate')\n",
    "        self.batch_slice = slice(self.sym_index * self.sym_batchsize, (self.sym_index + 1) * self.sym_batchsize)\n",
    "\n",
    "        self.sh_train_x = theano.shared(np.asarray(train_set[0], dtype=theano.config.floatX), borrow=True)\n",
    "        self.sh_train_t = theano.shared(np.asarray(train_set[1], dtype=theano.config.floatX), borrow=True)\n",
    "        self.sh_test_x = theano.shared(np.asarray(test_set[0], dtype=theano.config.floatX), borrow=True)\n",
    "        self.sh_test_t = theano.shared(np.asarray(test_set[1], dtype=theano.config.floatX), borrow=True)\n",
    "        self.sh_valid_x = theano.shared(np.asarray(validation_set[0], dtype=theano.config.floatX), borrow=True)\n",
    "        self.sh_valid_t = theano.shared(np.asarray(validation_set[1], dtype=theano.config.floatX), borrow=True)\n",
    "        \n",
    "        # Define the layers for the density estimation used in the lower bound.\n",
    "        l_log_pa = GaussianMarginalLogDensityLayer(self.l_a_mu, self.l_a_logvar)\n",
    "        l_log_pz = GaussianMarginalLogDensityLayer(self.l_z_mu, self.l_z_logvar)\n",
    "        l_log_qa_x = GaussianMarginalLogDensityLayer(1, self.l_a_logvar)\n",
    "        l_log_qz_xy = GaussianMarginalLogDensityLayer(1, self.l_z_logvar)\n",
    "        l_log_qy_ax = MultinomialLogDensityLayer(self.l_y, self.l_y_in, eps=1e-8)\n",
    "        if self.x_dist == 'bernoulli':\n",
    "            l_px_zy = BernoulliLogDensityLayer(self.l_xhat, self.l_x_in)\n",
    "        elif self.x_dist == 'gaussian':\n",
    "            l_px_zy = GaussianLogDensityLayer(self.l_x_in, self.l_xhat_mu, self.l_xhat_logvar)\n",
    "\n",
    "        ### Compute lower bound for labeled data_preparation ###\n",
    "        out_layers = [l_log_pa, l_log_pz, l_log_qa_x, l_log_qz_xy, l_px_zy, l_log_qy_ax]\n",
    "        inputs = {self.l_x_in: self.sym_x_l, self.l_y_in: self.sym_t_l}\n",
    "        log_pa_l, log_pz_l, log_qa_x_l, log_qz_axy_l, log_px_zy_l, log_qy_ax_l = get_output(out_layers, inputs)\n",
    "        py_l = softmax(T.zeros((self.sym_x_l.shape[0], self.n_y)))  # non-informative prior\n",
    "        log_py_l = -categorical_crossentropy(py_l, self.sym_t_l).reshape((-1, 1)).dimshuffle((0, 'x', 'x', 1))\n",
    "        lb_l = log_pa_l + log_pz_l + log_py_l + log_px_zy_l - log_qa_x_l - log_qz_axy_l\n",
    "        # Upscale the discriminative term with a weight.\n",
    "        log_qy_ax_l *= self.sym_beta\n",
    "        xhat_grads_l = T.grad(lb_l.mean(axis=(1, 2)).sum(), self.xhat_params)\n",
    "        y_grads_l = T.grad(log_qy_ax_l.mean(axis=(1, 2)).sum(), self.y_params)\n",
    "        lb_l += log_qy_ax_l\n",
    "        lb_l = lb_l.mean(axis=(1, 2))\n",
    "\n",
    "        ### Compute lower bound for unlabeled data_preparation ###\n",
    "        bs_u = self.sym_x_u.shape[0]  # size of the unlabeled data_preparation.\n",
    "        t_eye = T.eye(self.n_y, k=0)  # ones in diagonal and 0's elsewhere (bs x n_y).\n",
    "        # repeat unlabeled t the number of classes for integration (bs * n_y) x n_y.\n",
    "        t_u = t_eye.reshape((self.n_y, 1, self.n_y)).repeat(bs_u, axis=1).reshape((-1, self.n_y))\n",
    "        # repeat unlabeled x the number of classes for integration (bs * n_y) x n_x\n",
    "        x_u = self.sym_x_u.reshape((1, bs_u, self.n_x)).repeat(self.n_y, axis=0).reshape((-1, self.n_x))\n",
    "        out_layers = [l_log_pa, l_log_pz, l_log_qa_x, l_log_qz_xy, l_px_zy]\n",
    "        inputs = {self.l_x_in: x_u, self.l_y_in: t_u}\n",
    "        log_pa_u, log_pz_u, log_qa_x_u, log_qz_axy_u, log_px_zy_u = get_output(out_layers, inputs)\n",
    "        py_u = softmax(T.zeros((bs_u * self.n_y, self.n_y)))  # non-informative prior.\n",
    "        log_py_u = -categorical_crossentropy(py_u, t_u).reshape((-1, 1)).dimshuffle((0, 'x', 'x', 1))\n",
    "        lb_u = log_pa_u + log_pz_u + log_py_u + log_px_zy_u - log_qa_x_u - log_qz_axy_u\n",
    "        lb_u = lb_u.reshape((self.n_y, self.sym_samples, 1, bs_u)).transpose(3, 1, 2, 0).mean(\n",
    "            axis=(1, 2))  # mean over samples.\n",
    "        y_ax_u = get_output(self.l_y, self.sym_x_u)\n",
    "        y_ax_u = y_ax_u.mean(axis=(1, 2))  # bs x n_y\n",
    "        y_ax_u += 1e-8  # ensure that we get no NANs.\n",
    "        y_ax_u /= T.sum(y_ax_u, axis=1, keepdims=True)\n",
    "        xhat_grads_u = T.grad((y_ax_u * lb_u).sum(axis=1).sum(), self.xhat_params)\n",
    "        lb_u = (y_ax_u * (lb_u - T.log(y_ax_u))).sum(axis=1)\n",
    "        y_grads_u = T.grad(lb_u.sum(), self.y_params)\n",
    "\n",
    "        # Loss - regularizing with weight priors p(theta|N(0,1)) and clipping gradients\n",
    "        y_weight_priors = 0.0\n",
    "        for p in self.y_params:\n",
    "            if 'W' not in str(p):\n",
    "                continue\n",
    "            y_weight_priors += log_normal(p, 0, 1).sum()\n",
    "        y_weight_priors_grad = T.grad(y_weight_priors, self.y_params, disconnected_inputs='ignore')\n",
    "\n",
    "        xhat_weight_priors = 0.0\n",
    "        for p in self.xhat_params:\n",
    "            if 'W' not in str(p):\n",
    "                continue\n",
    "            xhat_weight_priors += log_normal(p, 0, 1).sum()\n",
    "        xhat_weight_priors_grad = T.grad(xhat_weight_priors, self.xhat_params, disconnected_inputs='ignore')\n",
    "\n",
    "        n = self.sh_train_x.shape[0].astype(theano.config.floatX)  # no. of data_preparation points in train set\n",
    "        n_b = n / self.sym_batchsize.astype(theano.config.floatX)  # no. of batches in train set\n",
    "        y_grads = [T.zeros(p.shape) for p in self.y_params]\n",
    "        for i in range(len(y_grads)):\n",
    "            y_grads[i] = (y_grads_l[i] + y_grads_u[i])\n",
    "            y_grads[i] *= n_b\n",
    "            y_grads[i] += y_weight_priors_grad[i]\n",
    "            y_grads[i] /= -n\n",
    "\n",
    "        xhat_grads = [T.zeros(p.shape) for p in self.xhat_params]\n",
    "        for i in range(len(xhat_grads)):\n",
    "            xhat_grads[i] = (xhat_grads_l[i] + xhat_grads_u[i])\n",
    "            xhat_grads[i] *= n_b\n",
    "            xhat_grads[i] += xhat_weight_priors_grad[i]\n",
    "            xhat_grads[i] /= -n\n",
    "\n",
    "        params = self.y_params + self.xhat_params\n",
    "        grads = y_grads + xhat_grads\n",
    "\n",
    "        # Collect the lower bound and scale it with the weight priors.\n",
    "        elbo = ((lb_l.sum() + lb_u.sum()) * n_b + y_weight_priors + xhat_weight_priors) / -n\n",
    "\n",
    "        clip_grad, max_norm = 1, 5\n",
    "        mgrads = total_norm_constraint(grads, max_norm=max_norm)\n",
    "        mgrads = [T.clip(g, -clip_grad, clip_grad) for g in mgrads]\n",
    "        sym_beta1 = T.scalar('beta1')\n",
    "        sym_beta2 = T.scalar('beta2')\n",
    "        updates = adam(mgrads, params, self.sym_lr, sym_beta1, sym_beta2)\n",
    "\n",
    "        ### Compile training function ###\n",
    "        x_batch_l = self.sh_train_x[self.batch_slice][:self.sym_bs_l]\n",
    "        x_batch_u = self.sh_train_x[self.batch_slice][self.sym_bs_l:]\n",
    "        t_batch_l = self.sh_train_t[self.batch_slice][:self.sym_bs_l]\n",
    "        if self.x_dist == 'bernoulli':  # Sample bernoulli input.\n",
    "            x_batch_u = self._srng.binomial(size=x_batch_u.shape, n=1, p=x_batch_u, dtype=theano.config.floatX)\n",
    "            x_batch_l = self._srng.binomial(size=x_batch_l.shape, n=1, p=x_batch_l, dtype=theano.config.floatX)\n",
    "        givens = {self.sym_x_l: x_batch_l,\n",
    "                  self.sym_x_u: x_batch_u,\n",
    "                  self.sym_t_l: t_batch_l}\n",
    "        inputs = [self.sym_index, self.sym_batchsize, self.sym_bs_l, self.sym_beta,\n",
    "                  self.sym_lr, sym_beta1, sym_beta2, self.sym_samples]\n",
    "        f_train = theano.function(inputs=inputs, outputs=[elbo], givens=givens, updates=updates)\n",
    "        \n",
    "        ### Compile testing function ###\n",
    "        class_err_test = self._classification_error(self.sym_x_l, self.sym_t_l)\n",
    "        givens = {self.sym_x_l: self.sh_test_x,\n",
    "                  self.sym_t_l: self.sh_test_t}\n",
    "        f_test = theano.function(inputs=[self.sym_samples], outputs=[class_err_test], givens=givens)\n",
    "        \n",
    "        ### Compile validation function ###\n",
    "        class_err_valid = self._classification_error(self.sym_x_l, self.sym_t_l)\n",
    "        givens = {self.sym_x_l: self.sh_valid_x,\n",
    "                  self.sym_t_l: self.sh_valid_t}\n",
    "        inputs = [self.sym_samples]\n",
    "        f_validate = theano.function(inputs=[self.sym_samples], outputs=[class_err_valid], givens=givens)\n",
    "        \n",
    "        return f_train, f_test, f_validate\n",
    "    \n",
    "    def _classification_error(self, x, t):\n",
    "        y = get_output(self.l_y, x, deterministic=True).mean(axis=(1, 2))  # Mean over samples.\n",
    "        t_class = T.argmax(t, axis=1)\n",
    "        y_class = T.argmax(y, axis=1)\n",
    "        missclass = T.sum(T.neq(y_class, t_class))\n",
    "        return (missclass.astype(theano.config.floatX) / t.shape[0].astype(theano.config.floatX)) * 100.\n",
    "\n",
    "    def get_output(self, x, samples=1):\n",
    "        return self.f_y(x, samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class GaussianMarginalLogDensityLayer(MergeLayer):\n",
    "    def __init__(self, mu, var, **kwargs):\n",
    "        self.mu, self.var = None, None\n",
    "        if not isinstance(mu, Layer):\n",
    "            self.mu, mu = mu, None\n",
    "        if not isinstance(var, Layer):\n",
    "            self.var, var = var, None\n",
    "        input_lst = [i for i in [mu, var] if not i is None]\n",
    "        super(GaussianMarginalLogDensityLayer, self).__init__(input_lst, **kwargs)\n",
    "\n",
    "    def get_output_shape_for(self, input_shapes):\n",
    "        return input_shapes[0]\n",
    "\n",
    "    def get_output_for(self, input, **kwargs):\n",
    "        mu = self.mu if self.mu is not None else input.pop(0)\n",
    "        logvar = self.var if self.var is not None else input.pop(0)\n",
    "\n",
    "        if mu == 1:\n",
    "            density = -0.5 * (T.log(2 * np.pi) + 1 + logvar)\n",
    "        else:\n",
    "            density = -0.5 * (T.log(2 * np.pi) + (T.sqr(mu) + T.exp(logvar)))\n",
    "        return T.sum(density, axis=-1, keepdims=True)\n",
    "\n",
    "\n",
    "class BernoulliLogDensityLayer(MergeLayer):\n",
    "    def __init__(self, x_mu, x, eps=1e-6, **kwargs):\n",
    "        input_lst = [x_mu]\n",
    "        self.eps = eps\n",
    "        self.x = None\n",
    "\n",
    "        if not isinstance(x, Layer):\n",
    "            self.x, x = x, None\n",
    "        else:\n",
    "            input_lst += [x]\n",
    "        super(BernoulliLogDensityLayer, self).__init__(input_lst, **kwargs)\n",
    "\n",
    "    def get_output_shape_for(self, input_shapes):\n",
    "        return input_shapes[0]\n",
    "\n",
    "    def get_output_for(self, input, **kwargs):\n",
    "        x_mu = input.pop(0)\n",
    "        x = self.x if self.x is not None else input.pop(0)\n",
    "\n",
    "        if x_mu.ndim > x.ndim:  # Check for sample dimensions.\n",
    "            x = x.dimshuffle((0, 'x', 'x', 1))\n",
    "\n",
    "        x_mu = T.clip(x_mu, self.eps, 1 - self.eps)\n",
    "        density = T.sum(-T.nnet.binary_crossentropy(x_mu, x), axis=-1, keepdims=True)\n",
    "        return density\n",
    "\n",
    "\n",
    "class MultinomialLogDensityLayer(MergeLayer):\n",
    "    def __init__(self, x_mu, x, eps=1e-8, **kwargs):\n",
    "        input_lst = [x_mu]\n",
    "        self.eps = eps\n",
    "        self.x = None\n",
    "        if not isinstance(x, Layer):\n",
    "            self.x, x = x, None\n",
    "        else:\n",
    "            input_lst += [x]\n",
    "        super(MultinomialLogDensityLayer, self).__init__(input_lst, **kwargs)\n",
    "\n",
    "    def get_output_shape_for(self, input_shapes):\n",
    "        return input_shapes[0]\n",
    "\n",
    "    def get_output_for(self, input, **kwargs):\n",
    "        x_mu = input.pop(0)\n",
    "        x = self.x if self.x is not None else input.pop(0)\n",
    "\n",
    "        # Avoid Nans\n",
    "        x_mu += self.eps\n",
    "\n",
    "        if x_mu.ndim > x.ndim:  # Check for sample dimensions.\n",
    "            x = x.dimshuffle((0, 'x', 'x', 1))\n",
    "\n",
    "        density = -(-T.sum(x * T.log(x_mu), axis=-1, keepdims=True))\n",
    "        return density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from data_preparation import mnist\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train a auxiliary deep generative model on the mnist dataset with 100 evenly distributed labels.\n",
    "\"\"\"\n",
    "n_labeled = 100  # The total number of labeled data points.\n",
    "n_samples = 100  # The number of sampled labeled data points for each batch.\n",
    "n_batches = 100  # The number of batches.\n",
    "mnist_data = mnist.load_semi_supervised(n_batches=n_batches, n_labeled=n_labeled, n_samples=n_samples,\n",
    "                                        filter_std=0.0, seed=123456, train_valid_combine=True)\n",
    "\n",
    "n, n_x = mnist_data[0][0].shape  # Datapoints in the dataset, input features.\n",
    "bs = n / n_batches  # The batchsize.\n",
    "\n",
    "# Initialize the auxiliary deep generative model.\n",
    "model = ADGMSSL(n_x=n_x, n_a=100, n_z=100, n_y=10, a_hidden=[500],\n",
    "                z_hidden=[500], xhat_hidden=[500], y_hidden=[500],\n",
    "                trans_func=rectify, x_dist='bernoulli')\n",
    "\n",
    "# Get the training functions.\n",
    "f_train, f_test, f_validate = model.build_model(*mnist_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch,time,train[lb],test[err]];1;126.29;288.9806;25.24%\n",
      "[epoch,time,train[lb],test[err]];2;122.80;236.9604;23.73%\n",
      "[epoch,time,train[lb],test[err]];3;122.12;216.5454;24.34%\n",
      "[epoch,time,train[lb],test[err]];4;121.49;205.2796;24.06%"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import cPickle as pkl\n",
    "import os\n",
    "path = \"output/notebook\"\n",
    "if not os.path.exists(path):\n",
    "    os.mkdir(path)\n",
    "\n",
    "bs_l = n_samples\n",
    "beta = 1200.\n",
    "lr = 3e-4\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "samples = 1\n",
    "epochs = 200\n",
    "for epoch in range(epochs):\n",
    "    train_outputs = []\n",
    "    start = time.time()\n",
    "    for i in xrange(n_batches):\n",
    "        train_output = f_train(i, bs, bs_l, beta, lr, beta1, beta2, samples)\n",
    "        train_outputs.append(train_output)\n",
    "    lb = np.mean(np.array(train_outputs), axis=0)\n",
    "    end = time.time() - start\n",
    "    \n",
    "    test_outputs = f_test(samples)\n",
    "    class_err_test = np.mean(np.array(test_outputs), axis=0)\n",
    "    \n",
    "    # model_params = [param.get_value() for param in model]\n",
    "    # pkl.dump(model_params, open(os.path.join(path, \"model.pkl\"), \"wb\"))\n",
    "    \n",
    "    print \"[epoch,time,train[lb],test[err]];%i;%0.2f;%0.4f;%0.2f%%\" % (epoch+1, end, lb, class_err_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
